{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebooks/RandomForest.ipynb\n",
    "\n",
    "## Import necessary libraries\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "from mongo_connection import get_matches_collection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from MongoDB\n",
    "collection = get_matches_collection()\n",
    "cursor = collection.find({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert MongoDB cursor to DataFrame\n",
    "df = pd.DataFrame(list(cursor))\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df['winner'] = df.apply(lambda row: 'Draw' if row['home_score'] == row['away_score'] else (row['home_team'] if row['home_score'] > row['away_score'] else row['away_team']), axis=1)\n",
    "\n",
    "# Converting categorical columns to numerical (one-hot encoded)\n",
    "df = pd.get_dummies(df, columns=['home_team', 'away_team', 'tournament', 'city', 'country', 'winner'], drop_first=True)\n",
    "\n",
    "# Defining features and target variable\n",
    "features = df.drop(columns=['_id', 'date', 'home_score', 'away_score', 'winner_Draw', 'winner_England', 'winner_Scotland'])\n",
    "target = df['winner_Draw'] \n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Model Training\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Confusion Matrix Visualization\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred, cmap='Oranges_r', normalize='true')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Feature Importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Feature Importance Visualization\n",
    "top_n = 20  # Number of top features to display\n",
    "sorted_importances = feature_importance_df.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=sorted_importances, palette='viridis')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and Plot ROC Curve\n",
    "y_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "auc_score = roc_auc_score(y_test, y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved ROC Curve Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}', color='darkorange')\n",
    "plt.plot([0, 1], [0, 1], 'k--', color='gray')  # Diagonal line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Precision-Recall Curve Visualization\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.', color='blue')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Cross-Validation\n",
    "cv_scores = cross_val_score(rf_model, features, target, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean Cross-Validation Accuracy:\", np.mean(cv_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Cross-Validation Results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(cv_scores) + 1), cv_scores, marker='o', linestyle='-', color='green')\n",
    "plt.axhline(y=np.mean(cv_scores), color='red', linestyle='--', label='Mean Accuracy')\n",
    "plt.xlabel('Fold Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Cross-Validation Scores')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Model Persistence\n",
    "\n",
    "# If models dir doesn't exist, create it\n",
    "models_dir = Path('../models')\n",
    "\n",
    "if not (models_dir.exists()):\n",
    "    os.mkdir(\"../models\", 0o755)\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(rf_model, '../models/random_forest_model.pkl')\n",
    "\n",
    "# Load the trained model from the file\n",
    "loaded_rf_model = joblib.load('../models/random_forest_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
